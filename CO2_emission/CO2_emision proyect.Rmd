---
title: "CO2_emissions"
author: "Anabella"
date: "July 2023"

---
CO2 emissions
The dataset was downloaded from
[Kaggle](https://www.kaggle.com/datasets/koustavghosh149/co2-emission-around-the-world)

Through this project, I aspire to showcase my proficiency in handling small datasets efficiently and effectively, proving that valuable insights can be extracted even with limited resources. I will make a basic EDA with R.

```{r}
#Load all the libraries
library(ggplot2)
library(gganimate)
library(transformr)
library(dplyr)
library(RColorBrewer)
library(agricolae)
library(tidyverse)
library(viridis)
library(tidyr)
library(fuzzyjoin)
library(ggstream)
library(colorspace)
library(ggtext)
library(cowplot)

```


Import dataset
```{r}
df <- read.csv("~/2. Anabella/4.GITHUB/Proyects_with_R/CO2_emission/CO2_emission.csv")
str(df)

```
## DATA CLEANING  

### Missing values
```{r}
# Count the number of null cells in the data frame
num_null_cells <- sum(is.na(df))

# Print the result
cat("Number of null cells:", num_null_cells, "\n")
```
Te dataframe has 770 null cells. But how many rows have at least one missing value and how many have no value at all?

```{r}
# Count the number of null rows in the data frame
num_null_rows <- sum(apply(is.na(df[,5:35]), 1, all))

# Count the number of rows with at least one NA value
num_rows_withNA <- sum(apply(is.na(df[,5:35]), 1, any))

# Count the number of null columns in the data frame
num_null_cols <- sum(apply(is.na(df[,5:35]), 2, all))

# Count the number of columns with at least one NA value
num_cols_withNA <- sum(apply(is.na(df[,5:35]), 2, any))

# Print the result
cat("Number of null rows:", num_null_rows, "\n")
cat("Number of rows with NA:", num_rows_withNA, "\n")
cat("Number of null columns:", num_null_cols, "\n")
cat("Number of col with NA:", num_cols_withNA, "\n")

```
Next I will delete the 24 null rows from the dataset.
```{r}
df_clean <- df[-which(apply(is.na(df[,5:35]), 1, all)), ]
str(df_clean)
```

```{r}
# Count the number of null rows in the data frame
num_null_rows <- sum(apply(is.na(df_clean[,5:35]), 1, all))

# Count the number of rows with at least one NA value
num_rows_withNA <- sum(apply(is.na(df_clean[,5:35]), 1, any))

# Count the number of null columns in the data frame
num_null_cols <- sum(apply(is.na(df_clean[,5:35]), 2, all))

# Count the number of columns with at least one NA value
num_cols_withNA <- sum(apply(is.na(df_clean[,5:35]), 2, any))

# Print the result
cat("Number of null rows:", num_null_rows, "\n")
cat("Number of rows with NA:", num_rows_withNA, "\n")
cat("Number of null columns:", num_null_cols, "\n")
cat("Number of col with NA:", num_cols_withNA, "\n")


```
I have succesfully deleted the null rows and there are still the 8 rows with missing values. How many missing values have each of these 8 rows?

```{r}
#To visualize the 8 rows with NAs. 
rows_with_missing_values <- df_clean[!complete.cases(df_clean), ]

# Count the number of null cells in the data frame
num_null_cells <- sum(is.na(df_clean))

# Print the result
cat("Number of null cells:", num_null_cells, "\n")
```
The 8 rows (coutries)  have 26 missing values in total.


## Data preprocessing
Now I will Remove 2019b (repeted column) and Indicator.name columns

```{r}
#remove the last column
df_clean <- df_clean[, -ncol(df)]

# delete the Indicator.name column
df_clean <- subset(df_clean, select= -Indicator.Name) 

df_clean
```

df[, -ncol(df)] subsets the original data frame df by selecting all rows and all columns except the last column. The resulting data frame has the same number of rows as the original, but one less column.

Rename column names

```{r}
colnames(df_clean)[colnames(df_clean) %in% c("Country.Name", "Region")] <- c("country_name", "region")

df_clean
```
- I will rearrage the dataset from wide to long format, creating separate rows for each year. 

```{r}
#library(tidyr)

# Reshape the dataset using tidyr::pivot_longer
long_df <- df_clean[, -ncol(df_clean)] %>%
  pivot_longer(cols = -c(country_name, country_code, region),
               names_to = "year",
               values_to = "co2_emissions_metric_tons_per_capita")

# remove the "X" prefix from the year column and convert it to a numeric data type
long_df$year <- as.numeric(gsub("X", "", long_df$year))

#check the structure of the df 
str(long_df)

# Count the number of null cells in the data frame
num_null_cells <- sum(is.na(long_df))

# Print the result
cat("Number of null cells in the long data frame is:", num_null_cells, "\n")

```
## DATA ANALYSIS  
```{r}
summary (long_df)

```

This is interestying. I can see that the mean CO2 emissions is arround 4 mt per capita but the maximun emissions is 50tm per capita !. I will plot this trend for a better understanding.

```{r}
world_summary <- long_df %>%
  group_by(year) %>%
  summarise(
    mean_co2 = mean(co2_emissions_metric_tons_per_capita, na.rm = TRUE),
    median_co2 = median(co2_emissions_metric_tons_per_capita, na.rm = TRUE),
    max_co2 = max(co2_emissions_metric_tons_per_capita, na.rm = TRUE),
    min_co2 = min(co2_emissions_metric_tons_per_capita, na.rm = TRUE)#Setting na.rm = TRUE tells R to remove the NA values before calculating the mean. IMPORTANT!
  )

```


Calculate country means in the df_clean data frame.
The NA values are ignored in the row mean calculations, so the resulting "country_mean" column will not contain any NA values.


```{r}
country_summary <- long_df %>%
  group_by(region, country_name) %>%
  summarise(
    mean_co2 = mean(co2_emissions_metric_tons_per_capita, na.rm = TRUE),
    median_co2 = median(co2_emissions_metric_tons_per_capita, na.rm = TRUE),
    max_co2 = max(co2_emissions_metric_tons_per_capita, na.rm = TRUE),
    min_co2 = min(co2_emissions_metric_tons_per_capita, na.rm = TRUE)) %>% #Setting na.rm = TRUE tells R to remove the NA values before calculating the mean. IMPORTANT!
    arrange(desc(mean_co2))
     
region_summary <- long_df %>%
  group_by(region) %>%
  summarise( 
    mean_co2 = mean(co2_emissions_metric_tons_per_capita, na.rm = TRUE),
    median_co2 = median(co2_emissions_metric_tons_per_capita, na.rm = TRUE),
    max_co2 = max(co2_emissions_metric_tons_per_capita, na.rm = TRUE),
    min_co2 = min(co2_emissions_metric_tons_per_capita, na.rm = TRUE)) %>% #Setting na.rm = TRUE tells R to remove the NA values before calculating the mean. IMPORTANT!
    arrange(desc(mean_co2)
  )

```

Countries and regions with the biggest CO2 emissions are 

```{r}
print(head(country_summary))

```


```{r}
print(region_summary[1:3,])
```
The 6 countries with the lowest CO2 emissions are

```{r}
tail(country_summary)
```

Lowest emissors
```{r}
region_summary[5:7,]
```

 
# DATA VIZ

LINE PLOT

```{r}
# Load the required libraries ( here I need ggplot2, gganimate, transformr )


# Create a line plot 
Fig1 <- ggplot(world_summary, aes(x = year)) +
  geom_line(aes(y = mean_co2, color = "Mean CO2 Emissions"), size = 1.5) +
  geom_line(aes(y = median_co2, color = "Median CO2 Emissions"), size = 1.5) +
  geom_line(aes(y = max_co2, color = "Max CO2 Emissions"), size = 1.5) +
  geom_line(aes(y = min_co2, color = "Min CO2 Emissions"), size = 1.5) +
  theme_minimal(base_size = 12)+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.background = element_rect(fill = "grey88", color = NA),
        panel.background = element_rect(fill = NA, color = NA))+
  labs(title = "CO2 Emissions Over Time",
       x = "Year",
       y = "CO2 mt per capita",
       color = "Emission Type") +
  scale_color_manual(values = c("red4","darkorange", "yellow", "deepskyblue4")) 

Fig1

```
```{r}
ggsave("Fig1.png", plot = Fig1, width = 10, height = 6, dpi = 600)

```

While the mean and minimun emissions have been stable over the years, the maximun emissions had a peak arround year 2000.

BAR PLOT

```{r}

# Here, we are using the "Dark2" color palette from RColorBrewer library

color_palette <- brewer.pal(n = 7, name = "Dark2")

Fig2 <- ggplot(country_summary, aes(x = reorder(country_name, +mean_co2), y = mean_co2, fill = region)) +
      geom_bar(stat = "identity") +  # Create the bar chart
      coord_flip() +  # Flip the x and y-axis to make a horizontal bar chart
      labs(title = "World mean CO2 emissions (1990-2019)",
      subtitle = "Each bar is one country",
      x = NULL, y = "Mean CO2 Emissions",
      fill = "World Region") +
      theme_minimal(base_size = 12) +
  theme (
  axis.text.x =NULL,
  axis.text.y = element_blank(),#I did not put the country labes because they would be unreadable
  plot.background = element_rect(fill = "grey88", color = NA),
  panel.background = element_rect(fill = NA, color = NA)
  )+
  scale_fill_manual(values = color_palette)  # Set the defined color palette

ggsave("Fig2.png", plot = Fig2, width = 10, height = 6, dpi = 600)

Fig2

```
This plot shows that Middle East& North Africa (pink) has the countries with more CO2 emissions followed by North america (green). Also, Sub-Saharan Africa countries are in the tail of the plot.  Lets support that with some statistic.


 **ANOVA test**
I will do a ANOVA test in order to see if there is statistical significance of the differences in means between the regions

```{r}
ANOVAmodel <- aov(mean_co2 ~ region, data = country_summary)
summary(ANOVAmodel)
```

The results show that there is a significant difference between at least two of the Regions means.
To see wich of the means are diferent I will make a LSD Fisher's test

**LSD Fisher's Test**  
*LSD (Least Significant Difference) Fisher test, also known as the Fisher's Least Significant Difference test, is a post-hoc test used after an ANOVA (Analysis of Variance) test to determine which pairs of group means are significantly different from each other. It compares the difference between two means to the average variability of all the means, as estimated from the ANOVA results.

The LSD test is useful when you have a significant ANOVA result with three or more groups, and you want to determine which specific groups are different from each other. It is a conservative test that adjusts for the multiple comparisons made between all possible pairs of group means. However, it assumes that the population variances are equal across all groups.

The LSD test works by calculating a critical value based on the alpha level (typically 0.05), the degrees of freedom within groups, and the error mean square from the ANOVA. Then, for each pair of means, it calculates the difference between them, and if the absolute value of this difference is greater than the critical value, it concludes that the means are significantly different.*

Load the library
```{r}
#library(agricolae)
```


```{r}
RegionLSD <- LSD.test(ANOVAmodel, "region", alpha = 0.05, console = TRUE)

```
The insight that this test gives is that Regions labeled with the same letter are not significantly different in terms of mean emissions. For example, "East Asia" and "Latin America" have different mean emissions (4.2 and 2.9, respectively), but they are both assigned the letter "d" which means their difference is not statistically significant (for a significance level of 0.05)

```{r}

RegionLSD$means


```

Here I can see  the dispersion or spread of the data points around the mean. A larger standard deviation indicates greater variability in the emissions within a region, while a smaller standard deviation suggests less variability and more consistency in the emissions. Although Middle East is the second region with more CO2 emissions, there is a lot of variability in the CO2 amissions among the countries of the different regions.
I am interested to plot the means with the SE instead of the STD. I will have to calculate it

```{r}

# Set up the PNG device for saving the plot
png("Fig3.png", width = 10, height = 6, units = "in", res = 600)

# Plot the base R plot
plot(RegionLSD, 
     variation="SD", 
     main = "Mean CO2 emissions by world region ± SD",
     ylab = "co2 mt emission per capita",
     ylim = c(0,25),
     las= 2,
    cex.names = 0.6)

# Close the device to save the plot as Fig3.png
dev.off()


```

BOX PLOT

With the results obtained from the ANOVA I will make a box plot
```{r}
#library(tidyverse) library(viridis) library(ggplot2)

bplot <- country_summary %>% 
  group_by(region) %>% # the subsequent plot will have separate box plots for each region.
  ggplot(aes(x = reorder(region, -mean_co2), y = mean_co2, fill = region))+
  stat_boxplot(
    geom = "errorbar",      # Bigotes
    width = 0.2) +
  geom_boxplot(
    outlier.colour = "red", # Color de los valores atípicos
    alpha = 0.9) +        
  geom_jitter(color="black", size=0.4, alpha=0.9) +  # Add jittered points for better visibility
  labs(x = "Regions", y = "Means", title = "Box Plot of Means by Region") +
  theme_minimal()+
  scale_fill_viridis(discrete = TRUE, alpha=0.4)+
    theme(
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        legend.position = "none",
     plot.background = element_rect(fill = "grey88", color = NA),
  panel.background = element_rect(fill = NA, color = NA))

ggsave("Fig3.png", plot = bplot, width = 10, height = 6, dpi = 600)
bplot

```
I can see the great dispersion of data in the Middle East and East Asia pacific


## STREAM GRAPH

Now I will do a data viz over time with the top 5 countries and the top 3 regions. This code was adapted from 
Cedric Scherer "TidyTuesday 2020/27 - X-Men by Claremont Run Project"
github link: https://github.com/z3tt/TidyTuesday/blob/main/R/2020_27_ClaremontRunXMen.Rmd


### Data definition

```{r}
top_countries_codes <- c("QAT", "ARE", "BHR")
top_countries <- filter(long_df, country_code %in% top_countries_codes)

top_regions <- c("North America", "Middle East & North Africa", "Europe & Central Asia")
top_regions <- filter(long_df, region %in% top_regions)

```


for this section i will use the packages tidyverse, fuzzyjoin, ggstream, colorspace, ggtext, cowplot


```{r}
#setting ggplot theme

theme_set(theme_minimal(base_size = 12))

theme_update(
  plot.title = element_text(
    size = 15,
    face = "bold",
    hjust = .5,
    margin = margin(10, 5, 30, 5)
  ),
  plot.caption = element_text(
    size = 9,
    color = "grey40",
    hjust = .5,
    margin = margin(20, 0, 5, 0)
  ),
  axis.text.y = element_blank(),
  axis.title = element_blank(),
  plot.background = element_rect(fill = "grey88", color = NA),
  panel.background = element_rect(fill = NA, color = NA),
  panel.grid = element_blank(),
  panel.spacing.y = unit(0, "lines"),
  strip.text.y = element_blank(),
  legend.position = "bottom",
  legend.text = element_text(size = 9, color = "grey40"),
  legend.box.margin = margin(t = 30), 
  legend.background = element_rect(
    color = "grey40", 
    size = .3, 
    fill = "grey95"
  ),
  legend.key.height = unit(.25, "lines"),
  legend.key.width = unit(2.5, "lines"),
  plot.margin = margin(rep(20, 4))
)
```

```{r}
# Define the color palette
reds_palette <- colorRampPalette(colors = c("#FFCCCC", "#FF6666", "#FF0000"))

# Generate the palette with three shades of red
pal <- reds_palette(3)

```
trick to make the start and end of the stream smoother.

```{r}
df_smooth_country <- top_countries %>%
  group_by(country_name, country_code, region) %>%
  slice(1:4) %>%
  mutate(
    year = c(
      min(top_countries$year) - 5,
      min(top_countries$year) - 1,
      max(top_countries$year) + 1,
      max(top_countries$year) + 5
      ),
    co2_emissions_metric_tons_per_capita = c(0, 5, 5, 0),
  )

df_smooth_region <- top_regions %>%
  group_by(region) %>%
  slice(1:4) %>%
  mutate(
    year = c(
      min(top_regions$year) - 5,
      min(top_regions$year) - 1,
      max(top_regions$year) + 1,
      max(top_regions$year) + 5
      ),
    co2_emissions_metric_tons_per_capita = c(0, 5, 5, 0),
  )

#bind dataframes
top_countries_smooth <- top_countries %>% bind_rows(df_smooth_country)

top_regions_smooth  <- top_regions %>% bind_rows(df_smooth_region)


#group countries by region

grouped_top_regions_smooth <- top_regions_smooth %>%
  group_by(region, year) %>%
  summarize(avg_co2_emissions = mean(co2_emissions_metric_tons_per_capita), .groups = "drop")

str(grouped_top_regions_smooth)

#are there missing values?
# Check for missing values in the avg_co2_emissions column
missing_indices <- which(is.na(grouped_top_regions_smooth$avg_co2_emissions))

# Get the years corresponding to the missing values
years_with_missing <- grouped_top_regions_smooth$year[missing_indices]
region_with_missing <- grouped_top_regions_smooth$region[missing_indices]
# Print the years with missing values

print(paste("In the year", years_with_missing, "there where missing values for", region_with_missing))
```


Plot

```{r}
g_country <- top_countries_smooth %>% 
  ggplot(
    aes(
      year, co2_emissions_metric_tons_per_capita, 
      color = country_name, 
      fill = country_name
    )
  ) +
  geom_stream(
    geom = "contour",
    color = "white",
    size = 1.25,
    bw = .45 # Controls smoothness
  ) +
  geom_stream(
    geom = "polygon",
    bw = .45,
    size = 0
  ) +
  scale_color_manual(
    expand = c(0, 0),
    values = pal,
    guide = "none"
  ) +
  scale_fill_manual(
    values = pal,
    name = NULL
  ) 

g_region <- grouped_top_regions_smooth %>% 
  ggplot(
    aes(
      year, avg_co2_emissions, 
      color = region, 
      fill = region
    )
  ) +
  geom_stream(
    geom = "contour",
    color = "white",
    size = 1.25,
    bw = .45 # Controls smoothness
  ) +
  geom_stream(
    geom = "polygon",
    bw = .45,
    size = 0
  ) +
  scale_color_manual(
    expand = c(0, 0),
    values = pal,
    guide = "none"
  ) +
  scale_fill_manual(
    values = pal,
    name = NULL
  ) 
  
  

g_country
g_region

```


### Title and captions
```{r}
g_country <- g_country +
  labs(
    title = "Top 3 countries with more CO2 emissions in metrictons per capita",
    caption = "Visualization by Anabella Varela  •  Data obtained from Kaggle • "
  ) 

g_country

g_region <- g_region +
  labs(
    title = "Top 3 regions with more CO2 emissions in metrictons per capita",
    caption = "Visualization by Anabella Varela  •  Data obtained from Kaggle • "
  ) 
g_region

```


### Add anotation of missing values for Middle East in the years 1992 to 1994

```{r}

g_region <- g_region +
    geom_vline(xintercept = 1992 : 1994, # draw vertical lines (xintercept) at specific x-coordinates. 
    color = "black", 
    size = .5,
    linetype = "dotted") +
  annotate(
    "text",
    x = 1993,
    y = -80,  # Adjust the y-coordinate to position the text at the bottom of the plot
    label = "Missing data",
    color = "black",
    size = 3,
    hjust = 0.5,  # Center the text horizontally
    vjust = -0.5,  # Place the text just below the plot
    family = "sans",  # Font family for the text
  ) 
  


g_region

```



```{r}

ggsave("Fig4.png", plot = g_country, width = 10, height = 6, dpi = 600)
ggsave("Fig5.png", plot = g_region, width = 10, height = 6, dpi = 600)



```

##Principal Component Analysis (PCA) 
PCA is a widely used statistical technique in data analysis. It is used for dimensionality reduction and pattern recognition in high-dimensional data. PCA transforms the original variables into a new set of uncorrelated variables called principal components, which capture the most significant variability in the data.
```{r}
###will make 
```









